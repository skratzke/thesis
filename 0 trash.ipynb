{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fccfdc0",
   "metadata": {},
   "source": [
    "# Costs and Time Overviews\n",
    "## Costs\n",
    "* Started with $5.58  \n",
    "* Ended up with $5.32 after 100 books \n",
    "* So $0,26 per 100 books\n",
    "* So for 17,000 books = $44,2\n",
    "\n",
    "## Time\n",
    "* 100 books take around 5 minutes, \n",
    "* so 17,000 books will take around 14 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e55169",
   "metadata": {},
   "source": [
    "Generell festzuhalten: Sumscore und proportions zeigen im grunde das gleiche Muster. \n",
    "\n",
    "Sum Score: \n",
    "------------------------\n",
    "\n",
    "                pos_sum   neg_sum  total_sum\n",
    "author_gender                               \n",
    "0              3.233743  5.510914  -2.277171\n",
    "1              4.102130  5.795434  -1.693303\n",
    "\n",
    "\n",
    "------------------------\n",
    "POS SUM\n",
    "author_gender              0         1\n",
    "master_genre                          \n",
    "Children            2.537037  3.338645\n",
    "Fantasy             3.505495  4.297003\n",
    "Historical Fiction  4.096916  4.862588\n",
    "Horror              3.013043  3.560976\n",
    "Literary Fiction    3.372059  3.793333\n",
    "Mystery             2.712018  3.031215\n",
    "Other               3.002488  4.102850\n",
    "Romance             3.611872  4.448276\n",
    "Science Fiction     2.814010  4.301538\n",
    "Thriller            2.955446  3.120448\n",
    "YA                  3.158824  3.723235\n",
    "author_gender\n",
    "0    3.161746\n",
    "1    3.870919\n",
    "dtype: float64\n",
    "-----------------------------\n",
    "NEG SUM \n",
    "author_gender              0         1\n",
    "master_genre                          \n",
    "Children            4.925926  4.601594\n",
    "Fantasy             7.008242  7.194142\n",
    "Historical Fiction  4.506608  5.253761\n",
    "Horror              5.034783  5.666667\n",
    "Literary Fiction    4.455882  4.837333\n",
    "Mystery             5.455782  6.915273\n",
    "Other               4.942786  5.423792\n",
    "Romance             4.086758  5.383029\n",
    "Science Fiction     6.500000  7.175385\n",
    "Thriller            6.328383  6.282913\n",
    "YA                  4.829412  5.127563\n",
    "author_gender\n",
    "0    5.279506\n",
    "1    5.805586\n",
    "dtype: float64\n",
    "__________________________________\n",
    "author_gender              0         1\n",
    "master_genre                          \n",
    "Children           -2.388889 -1.262948\n",
    "Fantasy            -3.502747 -2.897139\n",
    "Historical Fiction -0.409692 -0.391174\n",
    "Horror             -2.021739 -2.105691\n",
    "Literary Fiction   -1.083824 -1.044000\n",
    "Mystery            -2.743764 -3.884058 ####\n",
    "Other              -1.940299 -1.320942\n",
    "Romance            -0.474886 -0.934753 ####\n",
    "Science Fiction    -3.685990 -2.873846\n",
    "Thriller           -3.372937 -3.162465\n",
    "YA                 -1.670588 -1.404328\n",
    "author_gender\n",
    "0   -2.117760\n",
    "1   -1.934668\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e752c2",
   "metadata": {},
   "source": [
    "-> test for significance? \n",
    "wird bei männlichen und weiblichen authors unterschiedlich berichtet? unterschiedlich viele NAs? \n",
    "\n",
    "prop_pos for men would be lower if less NAs for stereotypical questions for men -> less NAs means less stereotypical stuff that there is no evidence for \n",
    "prop_pos for women would be higher if more NAs for stereotypical questions for women -> more NAs means more stereotypical stuff that there is no evidence for\n",
    "\n",
    "But when I looked at sum scores, where NAs are just not counted, female author also had a higher sum score: \n",
    "pos_sum   neg_sum  total_sum\n",
    "author_gender                               \n",
    "0              3.233743  5.510914  -2.277171\n",
    "1              4.102130  5.795434  -1.693303\n",
    "\n",
    "Maybe this is all explained by the fact that there are twice as many female authors as male authors and the biggest genre is romance and there women write stereotypically? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUM SCORE -------------------#########################\n",
    "\n",
    "# 1) Mean prop_pos per (genre, gender)\n",
    "by_genre = df.groupby(['master_genre','author_gender'])['neg_sum'].mean().unstack()\n",
    "print(by_genre)\n",
    "\n",
    "# 2) Average those genre‑specific means **equally**  \n",
    "balanced = by_genre.mean(axis=0)\n",
    "print(balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5544c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum Scores\n",
    "# Create sum scores \n",
    "\n",
    "# --- TOTAL SUM ---\n",
    "df['total_sum'] = df[answer_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "# --- POSITIVE SUM (only 1s from even-numbered answer columns) ---\n",
    "df['pos_sum'] = df[stereotypical_cols].apply(lambda row: (row == 1).sum(), axis=1)\n",
    "\n",
    "# --- NEGATIVE SUM (only -1s from odd-numbered answer columns) ---\n",
    "df['neg_sum'] = df[anti_stereotypical_cols].apply(lambda row: (row == -1).sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create averages\n",
    "\n",
    "# --- TOTAL AVERAGE ---\n",
    "df['total_avg'] = df[answer_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# --- POSITIVE SUM (only 1s from even-numbered answer columns) ---\n",
    "df['pos_avg'] = df[stereotypical_cols].apply(lambda row: (row == 1).mean(), axis=1)\n",
    "\n",
    "# --- NEGATIVE SUM (only -1s from odd-numbered answer columns) ---\n",
    "df['neg_avg'] = df[anti_stereotypical_cols].apply(lambda row: (row == -1).mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find 20 novels that I've read to check manually if FMC extraction works\n",
    "\n",
    "# # Original data with an extra column\n",
    "# books = [\n",
    "#     {\"Title\": \"Pride and Prejudice\", \"Author\": \"Jane Austen\", \"Female_Major_Character\": 1},  # Elizabeth Bennet\n",
    "#     {\"Title\": \"To Kill a Mockingbird\", \"Author\": \"Harper Lee\", \"Female_Major_Character\": 1},  # Scout Finch\n",
    "#     {\"Title\": \"The Book Thief\", \"Author\": \"Markus Zusak\", \"Female_Major_Character\": 1},  # Liesel Meminger\n",
    "#     {\"Title\": \"Twilight\", \"Author\": \"Stephenie Meyer\", \"Female_Major_Character\": 1},  # Bella Swan\n",
    "#     {\"Title\": \"The Fault in Our Stars\", \"Author\": \"John Green\", \"Female_Major_Character\": 1},  # Hazel Grace Lancaster\n",
    "#     {\"Title\": \"The Perks of Being a Wallflower\", \"Author\": \"Stephen Chbosky\", \"Female_Major_Character\": 1},  # Sam\n",
    "#     {\"Title\": \"Brave New World\", \"Author\": \"Aldous Huxley\", \"Female_Major_Character\": 1}, #Lenina Crowne\n",
    "#     {\"Title\": \"A Game of Thrones (A Song of Ice and Fire, #1)\", \"Author\": \"George R.R. Martin\", \"Female_Major_Character\": 1},  # Daenerys Targaryen, Arya Stark, Cersei Lannister, ...\n",
    "#     {\"Title\": \"The Lightning Thief\", \"Author\": \"Rick Riordan\", \"Female_Major_Character\": 1},  # Annabeth Chase\n",
    "#     {\"Title\": \"Lolita\", \"Author\": \"Vladimir Nabokov\", \"Female_Major_Character\": 1},  # Dolores \"Lolita\" Haze\n",
    "\n",
    "#     {\"Title\": \"The Hobbit and The Lord of the Rings\", \"Author\": \"J.R.R. Tolkien\", \"Female_Major_Character\": 0},\n",
    "#     {\"Title\": \"The Little Prince\", \"Author\": \"Antoine de Saint-Exupéry\", \"Female_Major_Character\": 0},\n",
    "#     {\"Title\": \"Crime and Punishment\", \"Author\": \"Fyodor Dostoevsky\", \"Female_Major_Character\": 0},  \n",
    "#     {\"Title\": \"Lord of the Flies\", \"Author\": \"William Golding\", \"Female_Major_Character\": 0}, \n",
    "#     {\"Title\": \"The Adventures of Huckleberry Finn\", \"Author\": \"Mark Twain\", \"Female_Major_Character\": 0}, \n",
    "#     {\"Title\": \"The Old Man and the Sea\", \"Author\": \"Ernest Hemingway\", \"Female_Major_Character\": 0}, \n",
    "#     {\"Title\": \"Alex Rider\", \"Author\": \"Anthony Horowitz\", \"Female_Major_Character\": 0},  \n",
    "#     {\"Title\": \"Der Trafikant\", \"Author\": \"Robert Seethaler\", \"Female_Major_Character\": 0}, \n",
    "# ]\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df_books = pd.DataFrame(books)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(df_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf069a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through books, 1 book = 1 request\n",
    "for index, row in df100.iterrows():\n",
    "    # Extract title and author from each row \n",
    "    title = row[\"title\"]\n",
    "    author = row[\"author\"]\n",
    "\n",
    "    # Format user prompt with current title and author\n",
    "    user_prompt = user_prompt_template.format(title=title, author=author)\n",
    "\n",
    "    # Payload\n",
    "    payload = {\n",
    "        \"model\": \"sonar\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 0,  # Controls randomness in the response\n",
    "        # \"top_p\": 0.9,\n",
    "        # \"search_domain_filter\": [\"<any>\"],\n",
    "        # \"return_images\": False,\n",
    "        # \"return_related_questions\": False,\n",
    "        # \"search_recency_filter\": \"<string>\",\n",
    "        # \"top_k\": 0,\n",
    "        # \"stream\": False,\n",
    "        # \"presence_penalty\": 0,\n",
    "        # \"frequency_penalty\": 1,\n",
    "        \"response_format\": {\n",
    "          \"type\": \"json_schema\",\n",
    "          \"json_schema\": {\"schema\": AnswerFormat.model_json_schema()},\n",
    "        },\n",
    "        # \"web_search_options\": {\"search_context_size\": \"high\"}\n",
    "    }\n",
    "\n",
    "    # Send the request to the Perplexity API\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        # Extract the answer from the response\n",
    "        answer = response_json.get('choices', [{}])[0].get('message', {}).get('content', 'No answer found')\n",
    "        # Extract sources\n",
    "        citation = response_json.get(\"citations\", [])\n",
    "        # Store answer and sources\n",
    "        responses.append(answer)\n",
    "        citations.append(citation)\n",
    "    else:\n",
    "        # Handle errors by appending an error message\n",
    "        responses.append(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    # # Sleep to avoid rate limiting\n",
    "    # time.sleep(1)\n",
    "\n",
    "# End timing the request process\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the time taken for the 100 requests\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for 100 books: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f378ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify answer format\n",
    "\n",
    "# from pydantic import RootModel\n",
    "# from typing import List, Literal, Tuple\n",
    "\n",
    "# class AnswerFormat(RootModel):\n",
    "#     root: Tuple[\n",
    "#         Literal[0, 1],  # 1st: 0 or 1\n",
    "#         int,            # 2nd: year\n",
    "#         List[str],      # 3rd: list of genres or tags\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1]\n",
    "#     ]  # Total = 29 items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0084a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response_format(row):\n",
    "    r = row['answer']\n",
    "\n",
    "    # Check that it's a list and has length 29\n",
    "    if not isinstance(r, list) or len(r) != 29:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # r[0] should be 0 or 1\n",
    "        if r[0] not in [0, 1]:\n",
    "            return False\n",
    "\n",
    "        # r[1] should be an int\n",
    "        if not isinstance(r[1], int):\n",
    "            return False\n",
    "\n",
    "        # r[2] should be a list of strings\n",
    "        if not isinstance(r[2], list):\n",
    "            return False\n",
    "        if not all(isinstance(tag, str) for tag in r[2]):\n",
    "            return False\n",
    "\n",
    "        # r[3:] should all be 0 or 1\n",
    "        if not all(isinstance(x, int) and x in [0, 1] for x in r[3:]):\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Issue in row: {row['title']} — Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "df_result['is_valid'] = df_result.apply(validate_response_format, axis=1)\n",
    "invalid_rows = df_result[~df_result['is_valid']]\n",
    "print(f\"Number of invalid rows: {len(invalid_rows)}\")\n",
    "print(invalid_rows[['title', 'answer']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify answer format\n",
    "\n",
    "# from pydantic import RootModel\n",
    "# from typing import List, Literal, Tuple\n",
    "\n",
    "# class AnswerFormat(RootModel):\n",
    "#     root: Tuple[\n",
    "#         Literal[0, 1, 99],  # 1st: 0 or 1\n",
    "#         int,            # 2nd: year\n",
    "#         List[str],      # 3rd: list of genres or tags\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1], Literal[0, 1],\n",
    "#         Literal[0, 1]\n",
    "#     ]  # Total = 29 items"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
